{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Vectorizing Gradient Descentâ€Šâ€”â€ŠMultivariate Linear Regression and Python implementation\n",
    "\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*lav3vWKSf34PvIFPiCaStg.jpeg)\n",
    "\n",
    "In this article, I shall go over the topic of arriving at the **Vectorized Gradient-Descent formulae for the Cost function of the for Matrix form of training-data Equations.** And along with that the Fundamentals of Calculus (especially Partial Derivative) and Matrix Derivatives necessary to understand the process.\n",
    "\n",
    "**So our target of this article is to understand the full Mathematics and the flow behind arriving at the below formulae**, which is the Vectorized Gradient of the training-data Matrix\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*JPmxQrvKmjcAxiURYOCYUA.png)\n",
    "\n",
    "\n",
    "## First a Refresher on basic MatrixÂ Algebra\n",
    "\n",
    "A matrix A over a field K or, simply, a matrix A (when K is implicit) is a rectangular array of scalars usually presented in the following form:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Z7Y4gBTgQMym-MVR7DZ8pw.png)\n",
    "\n",
    "\n",
    "The rows of such a matrix A are the m horizontal lists of scalars:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Ev4S0SCYrgKd7SP7wXitzg.png)\n",
    "\n",
    "\n",
    "and the columns of A are the n vertical lists of scalars:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Hy4T3QGYGiT2YPwzoMdTSQ.png)\n",
    "\n",
    "\n",
    "A matrix with m rows and n columns is called an m by n matrix, written m\\*n. The pair of numbers m and n is called the size of the matrix. Two matrices A and B are equal, written A = B, if they have the same size and if corresponding elements are equal. Thus, the equality of two m \\* n matrices is equivalent to a system of mn equalities, one for each corresponding pair of elements.\n",
    "A matrix with only one row is called a row matrix or row vector, and a matrix with only one column is called a column matrix or column vector. A matrix whose entries are all zero is called a zero matrix and will usually be denoted by 0.\n",
    "\n",
    "## Matrix Multiplication\n",
    "\n",
    "The below image is taken from Khan Academyâ€™s excellent linear algebra course.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*jYjtn1J2xM5iror3FXK4xQ.png)\n",
    "\n",
    "**In above, each entry in the product matrix is the dot product of a row in the first matrix and a column in the second matrix**\n",
    "\n",
    "**More explanation for higher dimension case** â€”â€ŠIf the product **AB = C** is defined, where C is denoted by \\[cij\\], then the\n",
    "element cij is obtained by multiplying the elements in the ith row of **A** by the corresponding elements in the jth column of **B** and adding. Thus, if **A** has order k \\* n, and **B** has order n \\* p then\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*fNTSrRCAcK_j4HvbLslqvA.png)\n",
    "\n",
    "\n",
    "**then c11 is obtained by multiplying the elements in the first row of A by the corresponding elements in the first column of B and adding; hence,**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*7RSD_Z9KmiX5EKemHPL_ww.png)\n",
    "\n",
    "\n",
    "**The element c12 is found by multiplying the elements in the first row of A by the corresponding elements in the second column of B and adding; hence,**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*UrH_UpFOZ6zZmv6BK8pJ4g.png)\n",
    "\n",
    "\n",
    "**The element ckp below is obtained by multiplying the elements in the kth row of A by the corresponding elements in the pth column of B and adding; hence,**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*x779-OyJDcpwkLpunSLusw.png)\n",
    "\n",
    "\n",
    "**There are four simple rules that will help us in multiplying matrices, listed here**\n",
    "\n",
    "1\\. Firstly, we can only multiply two matrices when the number of columns in\n",
    "matrix A is equal to the number of rows in matrix B.\n",
    "\n",
    "2\\. Secondly, the first row of matrix A multiplied by the first column of matrix B\n",
    "gives us the first element in the matrix AB, and so on.\n",
    "\n",
    "3\\. Thirdly, when multiplying, order mattersâ€Šâ€”â€Šspecifically, AB â‰  BA.\n",
    "\n",
    "4\\. Lastly, the element at row i, column j is the product of the ith row of matrix A and the jth column of matrix B.\n",
    "\n",
    "The order in which we multiply matters. We must keep the matrices\n",
    "in order, but we do have some flexibility. As we can see in the following equation, the parentheses can be moved:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*abGjdRm7NAazy9_-lmCqVg.png)\n",
    "\n",
    "\n",
    "The following three rules also apply for Matrix Operation.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*ZIYKAV-1BlirjaAPxuKbSA.png)\n",
    "\n",
    "\n",
    "Let's see an example of Matrix multiplication\n",
    "\n",
    "Another example of Matrix multiplication\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*8O9Z0rMLEfl3JyGJnqE_yA.png)\n",
    "\n",
    "\n",
    "Implementing a dot production with numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "_\\# this will create integer as array-elements\n",
    "_A = np.random.randint(10, _size_\\=(2,3))\n",
    "B = np.random.randint(10, _size_\\=(3,2))\n",
    "\n",
    "This will create 2 Matrices as below e.g.\n",
    "\\[\\[9 0 8\\] \\[3 5 5\\]\\]\n",
    "\n",
    "\\[\\[5 2\\]\n",
    " \\[0 7\\]\n",
    " \\[0 7\\]\n",
    "\\]\n",
    "\n",
    "print(np.dot(A, B))\n",
    "\n",
    "\\# Output\n",
    "\\[\\[45 74\\]\n",
    "\\[15 76\\]\\]\n",
    "\n",
    "#### The Hadamard Productâ€Šâ€”â€ŠOther Kinds of Matrix Multiplication\n",
    "\n",
    "Hadamard multiplication is defined for matrices of the same shape as the multiplication of each element of one matrix by the corresponding element of the other matrix. Hadamard multiplication is often denoted by as below, for two matrices A(nÃ—m) and B(nÃ—m) we have\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*MA5IGGGKTYlyqQq_GEwvOA.png)\n",
    "\n",
    "\n",
    "## Refresher on Multivariate Linear Regression\n",
    "\n",
    "#### First, start with the **Simple Linear Regression (SLR)**.\n",
    "\n",
    "Suppose we have 2 equations as below\n",
    "\n",
    "10 = 2x + 2y\n",
    "18 = 4x + y\n",
    "\n",
    "So the Matrix form\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*RyDyp9YR3izJPoWOfQN4Vg.png)\n",
    "\n",
    "\n",
    "**So in general Mathematic form for the single independent variable case**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*ks4aGLCIcSpl0OEzS9gW3g.png)\n",
    "\n",
    "\n",
    "So the set of equations for all the observation will be as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*FbGj_xW0feYo-VzVUCRaSg.png)\n",
    "\n",
    "\n",
    "And the Matrix form of which is below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*pf4gjU4ZPNzTYXBXNh2yUQ.png)\n",
    "\n",
    "\n",
    "So **Y** is n \\* 1 matrix, **X** is an \\* 2 matrix, **Î²** is 2 \\* 1 matrix\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*e8BUHsZOSP3fYxsmnWmJwQ.png)\n",
    "\n",
    "#### **Multiple Linear Regression (MLR)**\n",
    "\n",
    "Suppose that the response variable Y and at least one predictor variable xi are quantitative. Then the equation for a specific Y value under the **MLR** model is\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*z_Le5Iw_hnqBNAUtjxCyBg.png)\n",
    "\n",
    "\n",
    "for i = 1,Â .Â .Â .Â , n. Here n is the sample size and the random variable ei is the\n",
    "ith error. So\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*mU2qyHAz6l_sAPjiX2vivQ.png)\n",
    "\n",
    "\n",
    "Which is in compact notation below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*b0Mjk-rNWJzZVp1m6IeiRw.png)\n",
    "![](https://cdn-images-1.medium.com/max/800/1*LXE1X_GBi6w__cwn1DyPrg.png)\n",
    "\n",
    "\n",
    "**And now in matrix notation, these n sets of equations become**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*gytWOZW1m99KNjzv5dCNeg.png)\n",
    "\n",
    "\n",
    "where **Y** is the vector of the response variable and is an n Ã— 1 vector of dependent variables, **X** is the matrix of the k independent/explanatory variables (usually the first column is a column of ones for the constant term) and is an n Ã— p matrix of predictors, **Î²** is a p Ã— 1 vector of unknown coefficients, and e is an n Ã— 1 vector of unknown errors. Equivalently,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*xG70qjyrg_rOpk-mb0sfGg.png)\n",
    "\n",
    "\n",
    "Where\n",
    "**Y**Â : is output vector for n **training examples.\n",
    "X** : is matrix of size n**\\*p** where each ith row belongs to ith training set.\n",
    "**Î²**Â : is weight vector of size p for p training features.\n",
    "\n",
    "## Different types of Matrix Differentiation\n",
    "\n",
    "#### **1-Differentiation with Respect to aÂ Scalar**\n",
    "\n",
    "Differentiation of a structure (vector or matrix, for example) with respect to a scalar is quite simple; it just yields the ordinary derivative of each element of the structure in the same structure. **Thus, the derivative of a vector or a matrix with respect to a scalar variable is a vector or a matrix, respectively, of the derivatives of the individual elements**.\n",
    "\n",
    "**1.A-Derivatives of Vectors with Respect to Scalars**\n",
    "\n",
    "The derivative of the vector y(x) = (y1Â ,Â .Â .Â .Â , yn ) with respect to the scalar x\n",
    "is the vector\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*qC_wogA-j6jLveKF64dEXg.png)\n",
    "\n",
    "\n",
    "The second or higher derivative of a vector with respect to a scalar is likewise a vector of the derivatives of the individual elements; that is, it is an array of higher rank.\n",
    "\n",
    "**1.B-Derivatives of Matrices with Respect to Scalars**\n",
    "\n",
    "The derivative of the matrix **Y(x)** defined as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*ubqLah4DbO64Bulfd45-ag.png)\n",
    "\n",
    "\n",
    "with respect to the scalar, x is the matrix\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*rAlOr-wUTrAleLRYUmF8Aw.png)\n",
    "\n",
    "\n",
    "The second or higher derivative of a matrix with respect to a scalar is\n",
    "likewise a matrix of the derivatives of the individual elements.\n",
    "\n",
    "#### 1.Câ€Šâ€”â€ŠDerivatives of Functions with Respect toÂ Scalars\n",
    "\n",
    "Differentiation of a function of a vector or matrix that is linear in the elements\n",
    "of the vector or matrix involves just the differentiation of the elements, fol-\n",
    "lowed by application of the function. For example, the derivative of a trace of\n",
    "a matrix is just the trace of the derivative of the matrix. On the other hand,\n",
    "the derivative of the determinant of a matrix is not the determinant of the\n",
    "derivative of the matrix\n",
    "\n",
    "#### 1.Dâ€Šâ€”â€ŠHigher-Order Derivatives with Respect toÂ Scalars\n",
    "\n",
    "Because differentiation with respect to a scalar does not change the rank of the object (â€œrankâ€ here means rank of an array or â€œshapeâ€), higher-order derivatives\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*utNbveSxIpgXJUJqX9FZpw.png)\n",
    "\n",
    "\n",
    "with respect to scalars are merely objects of the same rank\n",
    "whose elements are the higher-order derivatives of the individual elements.\n",
    "\n",
    "#### 2-Differentiation with Respect to aÂ Vector\n",
    "\n",
    "Differentiation of a given object with respect to an n-vector yields a vector for each element of the given object. The basic expression for the derivative, from formula\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*q-p7BNUvoqlugltTiGtCVw.png)\n",
    "\n",
    "\n",
    "for an arbitrary conformable vector y. The arbitrary y indicates that the derivative is omnidirectional; it is the rate of change of a function of the vector in any direction.\n",
    "\n",
    "#### 2.Aâ€Šâ€”â€ŠDerivatives of Scalars with Respect to Vectors; TheÂ Gradient\n",
    "\n",
    "The derivative of a scalar-valued function with respect to a vector is a vector\n",
    "of the partial derivatives of the function with respect to the elements of the\n",
    "vector. If f (x) is a scalar function of the vector x = (x1Â ,Â .Â .Â .Â , xn ),\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*oP3EN-tXI9TxwyYr2hu25A.png)\n",
    "\n",
    "\n",
    "if those derivatives exist. This vector is called the gradient of the scalar-valued\n",
    "function, and is sometimes denoted by âˆ‡f (x)\n",
    "\n",
    "#### 2.Bâ€Šâ€”â€ŠDerivatives of Vectors with Respect to Vectors; TheÂ Jacobian\n",
    "\n",
    "The derivative of an m-vector-valued function of an n-vector argument consists of nm scalar derivatives. These derivatives could be put into various structures. Two obvious structures are an n Ã— m matrix and an m Ã— n matrix.\n",
    "\n",
    "For a function,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*RypMapKwOWkXzAmZ7lvtcA.png)\n",
    "\n",
    "\n",
    "we define\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*VpZYNCbTNSCnpXBxUW4zHA.png)\n",
    "\n",
    "\n",
    "to be the n Ã— m matrix, which is the natural extension of âˆ‚/âˆ‚x applied to a scalar function. The above the notation is more precise because it indicates that the elements of f correspond to the columns of the result.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*WUY3HJZ1eqHfCwceNvYDgA.png)\n",
    "\n",
    "\n",
    "if those derivatives exist. This derivative is called the matrix gradient and\n",
    "is denoted by âˆ‡f for the vector-valued function fÂ . (Note that the âˆ‡\n",
    "symbol can denote either a vector or a matrix, depending on whether the\n",
    "function being differentiated is scalar-valued or vector-valued.)\n",
    "\n",
    "**The m Ã— n matrix is called the Jacobian of f and is denoted by Jf as below**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*lxwoMi34O7yt9u4ac2yY7w.png)\n",
    "\n",
    "\n",
    "## General Form of theÂ Jacobian\n",
    "\n",
    "For arriving at the general Mathematical form of Jacobian I would refer a quite [well-recognized Paper](https://arxiv.org/pdf/1802.01528.pdf) in this field.\n",
    "\n",
    "Let **y = f(x)** be a vector of m scalar-valued functions that each take a vector x of length **n = |x|** where |x| is the cardinality (count) of elements in x. Each fi function within **f** returns a scalar just as in the previous section:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*uy7W1fPZHsBIRP68F-cplQ.png)\n",
    "\n",
    "\n",
    "For instance,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*wXAxCx51hA7gkj71OEIvdg.png)\n",
    "\n",
    "\n",
    "So the set of equations are as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*qVogW73Z3IYAjy1BND1d8Q.png)\n",
    "\n",
    "\n",
    "So we have m = n functions and parameters, in this case. Generally speaking, though, **the Jacobian matrix is the collection of all m Ã— n possible partial derivatives (m rows and n columns), which is the stack of m gradients with respect to x:**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*_VbzYvXGjMZVe4jNlcfaWg.png)\n",
    "\n",
    "\n",
    "Each of these\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*eTRerCjUn84a5t9Eyx4b8Q.png)\n",
    "\n",
    "\n",
    "is a horizontal n-vector because the partial derivative is with respect to a vector, x, whose length is n = |x|. The width of the Jacobian is n if weâ€™re taking the partial derivative with respect to x because there are n parameters we can wiggle, each potentially changing the functionâ€™s value. Therefore, the Jacobian is always m rows for m equations.\n",
    "\n",
    "## Now the Cost Function under Multivariate Linear Regression\n",
    "\n",
    "The equation for the hypothesis function is as follows\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*qp_vtZgLiCSNoU41L0eGow.png)\n",
    "\n",
    "\n",
    "The general notations that I will use for extending the above function\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*M7dNnqhtPHAWJekLPzo-8Q.png)\n",
    "\n",
    "\n",
    "So if we are predicting house-price with the above MLR equation, then _Î¸_0 will be the basic/base price of a house, then _Î¸_1 as the price per room, _Î¸_2 as the price per KM-distance from the nearest Airport.\n",
    "\n",
    "Using the definition of matrix multiplication, our multivariate hypothesis function can be concisely represented as:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*asQQI3lFzXRdBfrtX5y1MQ.png)\n",
    "\n",
    "\n",
    "This is a vectorization of our hypothesis function for one training example;\n",
    "\n",
    "Now, using the fact that for a vector z, we have that\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*XgaPuVFe817NZW4nlRDcuQ.png)\n",
    "\n",
    "\n",
    "Applying the above identity to the right-hand-side of the Cost function (below)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*B-qvrdYQDmdKxKminrxH-Q.png)\n",
    "\n",
    "\n",
    "So now the Cost function takes the following form\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*WUsD8CFsC5H78aouLZFs3w.png)\n",
    "\n",
    "\n",
    "And our target is to\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*6do1HGRtbK1C_u0nW5h58Q.png)\n",
    "\n",
    "\n",
    "Wher the thetas Î¸ are the weights, and the above partial derivative for any weights wj will be as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*sZi1c1yL_8gupQrJtgfazA.png)\n",
    "\n",
    "\n",
    "So the Gradient-Descent process for Multivariate case becomes\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*QluVOgLvQ4RB2_IBwE0EkQ.png)\n",
    "\n",
    "\n",
    "Where Î¸ and x are column vector given by\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Sej0ld0Ahn2jsvVv0t6-OQ.png)\n",
    "\n",
    "\n",
    "And that's why we take the transpose of Î¸ to multiply with column-vector x to get the hypothesis (as earlier mentioned in this article)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*asQQI3lFzXRdBfrtX5y1MQ.png)\n",
    "\n",
    "\n",
    "## Refresherâ€Šâ€”â€ŠMatrix-Derivative Identities required for the Mathematical Derivation of the Gradient of a Matrix w.r.t. toÂ Vectors\n",
    "\n",
    "The derivative of a matrix is usually referred to as the gradient and denoted as âˆ‡. Consider a function\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Ba_SG3OaqvgLJz0MO4oEwQ.png)\n",
    "\n",
    "\n",
    "That is the Matrix will be as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*UeisP-Ui6Na1LXX2ACqRCw.png)\n",
    "\n",
    "\n",
    "Then,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*2CVL0vtFozL8yFRja5-D1w.png)\n",
    "\n",
    "\n",
    "Thus, the gradient âˆ‡Af(A) is itself an m-by-n matrix, whose (i, j)-element is\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Mq9Lgie0lCtdNBf46FRzSA.png)\n",
    "\n",
    "\n",
    "For example, lets take a look at a very simple case. Suppose\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*sYpBmF4XVcpdI9b8VzP-OA.png)\n",
    "\n",
    "\n",
    "is a 2-by-2 matrix, and the function\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*kvUQK-Ndj6bkZfkaRZVp4Q.png)\n",
    "\n",
    "\n",
    "is given by\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*fqtrwQBn10hY3dD2tpPNpg.png)\n",
    "\n",
    "\n",
    "Here, Aij denotes the (i, j) entry of the matrix A. We then have\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*UoBIW03p-QR0EB8qaQkmvw.png)\n",
    "\n",
    "\n",
    "## Matrix Transpose Identities\n",
    "\n",
    "Each of the below identities can be proved separately mathematically proved.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*h6bwL_K_X97XdffTZxkR2w.png)\n",
    "\n",
    "\n",
    "Another related one, If ð´ and ðµ are two matrices of the same order, then\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*YbUQQ6xfwwf_e8xqs5K4Fg.png)\n",
    "\n",
    "\n",
    "#### The Trace: tr(Â·) of aÂ Matrix\n",
    "\n",
    "The sum of the diagonal elements of a square matrix is called the trace of the\n",
    "matrix. We use the notation â€œtr(A)â€ to denote the trace of the matrix A:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Pxqs5JaBfecbVkKFM1OMuw.png)\n",
    "![](https://cdn-images-1.medium.com/max/800/1*q6zf6VbQackvsBSmpujWYQ.png)\n",
    "\n",
    "\n",
    "Because of the associativity of matrix multiplication, this relation can be\n",
    "extended as\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*_pcySNgZxp8kqVAv5Onibg.png)\n",
    "\n",
    "\n",
    "## Now proceed to find the Gradient of the Cost function.\n",
    "\n",
    "#### First a Refresher on the Gradient Descent Algorithm\n",
    "\n",
    "To implement Gradient Descent, you need to compute the gradient of the cost function with regard to each model parameter Î¸j. In other words, you need to calculate how much the cost function will change if you change Î¸j just a little bit. This is called a partial derivative. It is like asking â€œWhat is the slope of the mountain under my feet if I face east?â€. and then asking the same question facing north.\n",
    "\n",
    "Now you would recognize the very well-known cost function\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*bvUpqMmn3Ui0xgXWtrOlcg.png)\n",
    "\n",
    "\n",
    "And the following the Jacobian identity discussed above the Gradient vector of the cost function will be\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*tc5cu5BRRmrB47FjtVAwAg.png)\n",
    "\n",
    "\n",
    "Notice that this formula involves calculations over the full training set **X**, at each Gradient Descent step! This is why the algorithm is called **Batch Gradient Descent**: it uses the whole batch of training data at every step.\n",
    "\n",
    "Once you have the gradient vector, which points uphill, just go in the opposite direction to go downhill. This means subtracting âˆ‡Î¸MSE(Î¸) from Î¸. This is where the learning rate Î· comes into play:5 multiply the gradient vector by Î· to determine the size of the downhill step\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*XIm_W2QAYrqJftCE42t0Dg.png)\n",
    "\n",
    "\n",
    "**Now repeating below section of the Matrix form of the training dataset, from our earlier part of this article â€”**\n",
    "\n",
    "The general form of multiple linear regression (MLR) model is\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*1SkKxwZLZy6SQvpuWkiQFA.png)\n",
    "\n",
    "\n",
    "for i = 1,Â .Â .Â .Â , n. Here n is the sample size and the random variable ei is the\n",
    "ith error. I**n matrix notation, these n sets of equations become**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*gytWOZW1m99KNjzv5dCNeg.png)\n",
    "\n",
    "The **Y** vector is the response variable and is an n Ã— 1 vector of dependent variables, **X** is the matrix of the k independent/explanatory variables (usually the first column is a column of ones for the constant term) and is an n Ã— p matrix of predictors, **Î²** is a p Ã— 1 vector of unknown coefficients, and e is an n Ã— 1 vector of unknown errors. Equivalently,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*xG70qjyrg_rOpk-mb0sfGg.png)\n",
    "\n",
    "\n",
    "Where\n",
    "**Y**Â : is output vector for n **training examples.\n",
    "X** : is matrix of size n**\\*p** where each ith row belongs to ith training set.\n",
    "**Î²**Â : is weight vector of size p for p training features.\n",
    "\n",
    "Note that Î² in the above is not a scalar, but a vector.\n",
    "\n",
    "Now we have the RSS defined as\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*typfSXiJDet6UwB2-9Vq-Q.png)\n",
    "\n",
    "\n",
    "## Alternative-1â€Šâ€”â€ŠVectorized Calculation of Gradient of our Matrix form trainingÂ data\n",
    "\n",
    "Note the above is directly derived from using the identity that for a vector z, we have\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*HRZOm73aCQkTwcHM8U817Q.png)\n",
    "![](https://cdn-images-1.medium.com/max/800/1*fkkkh9pthCgqJvZ29YuL4A.png)\n",
    "\n",
    "\n",
    "Now let,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*aw74yCdXcb8-GYx18UBGJA.png)\n",
    "\n",
    "\n",
    "Then for the following assumption\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*wCTijrmwU43BeoFujfX_eg.png)\n",
    "![](https://cdn-images-1.medium.com/max/800/1*Z13CkDXJtTTptNd9feSHGQ.png)\n",
    "\n",
    "\n",
    "Therefore,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*dqxnBP7PXrycxImjvHXaeg.png)\n",
    "\n",
    "\n",
    "**We have, for each of _k_\\=1,â€¦, _p_**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*iEoYbHv2PyKuOPy5GFe_zA.png)\n",
    "\n",
    "\n",
    "Then for the whole matrix (i.e. the whole set of training data set or the whole set of Hypothesis Equation ), we will get\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*CJ7VQA1EMOFGKmBuAHsGYw.png)\n",
    "\n",
    "\n",
    "Which in the final Vector form\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*RWEJfod0BlYgMSylSwr0MA.png)\n",
    "\n",
    "Note, that in the last equality, I had to get the Transpose of **X** because when doing matrix multiplicationâ€Šâ€”â€Šthat's a dot product of rows of the first matrix to columns of the second matrix. The number of columns of the 1st matrix must equal the number of rows of the 2nd matrix.\n",
    "\n",
    "So by transposing the _p-_th column of **X** ends up being the _p-_th row of the X-Transposed. Thus, when doing a dot product between the p-th row of **X-Transposed with (yâ€Šâ€”â€ŠXÎ²) it will match perfectly as I am** using all of the entries of the _p-_th column of **X**\n",
    "\n",
    "## Alternative-2â€Šâ€”â€ŠAnd below is an alternative calculation for arriving at the same Vectorized Gradient formulae for training-data in MatrixÂ form.\n",
    "\n",
    "Here, I am denoting the coefficients with **Î¸ or Theta (instead of Î² that we used above in our Alternative-1 Gradient Calculationâ€Šâ€”â€Šonly to make the presentation differentiable)**\n",
    "\n",
    "Again assume we have our Training Set of data as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*7CBrI6xXmiIGsP-BddHLDw.png)\n",
    "\n",
    "\n",
    "Also, let **y** be the m-dimensional vector containing all the target values from the training set:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*mTYp0mTm1eoBjGhl_oD70w.png)\n",
    "\n",
    "\n",
    "And we have the Predicted Value or the Hypothesized value as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*rcj6DVtHGYImNIlAPgtMBA.png)\n",
    "\n",
    "\n",
    "So we can say the below\n",
    "\n",
    "And now again, we need to use the same **vector identity** mentioned above, that for a vector z, we have\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*HRZOm73aCQkTwcHM8U817Q.png)\n",
    "\n",
    "\n",
    "Using the above we have the below relation for the Cost function\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*3dmZKXEIrhcpKhyYvky3fw.png)\n",
    "\n",
    "\n",
    "We have already introduced the trace operator of a Matrix, written as **â€œtr.â€** Now we need to use a couple of more **matrix derivatives Identities** (that I am just stating below here, and they all have robust Mathematical proofs, the details of which I am not including here).\n",
    "\n",
    "So below 2 M**atrix Derivative Identities hold true and we need to use them to arrive at the Gradient Calculation.**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*RwqJFodt5c5xx9QWo0_mJg.png)\n",
    "\n",
    "Combining the above two Equations or Identities we derive\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*6sTQO3HWCvb5kBK0SjaRrQ.png)\n",
    "\n",
    "\n",
    "So now Final Gradient Calculation will be as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*8vk4kGAQ1nr9uRTcRqtSQw.png)\n",
    "\n",
    "\n",
    "In the third step above, we used the fact that the trace of a real number is just the real number; the fourth step used the fact that\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*UmbyevOF6r-gTCmBvJMc0g.png)\n",
    "\n",
    "\n",
    "And the fifth step used below equation that we already mentioned\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*GBVfdoY691FvRI-g81uxew.png)\n",
    "\n",
    "\n",
    "And also the below Matrix Identity\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*3H0jOMR9C4U-ykwsjZHZ1Q.png)\n",
    "\n",
    "\n",
    "With\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*BFMwcbJMLRGmSesT5QpaJw.png)\n",
    "\n",
    "\n",
    "Take a note of the final result of the Gradient, which is the same form that we arrived at earlier under the Alternative-1 calculation of Gradient\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*0hjVo1G2QsfGjmj48vCe2Q.png)\n",
    "\n",
    "And above is the exact formulae that we will implement in Python/Numpy very soon below.\n",
    "\n",
    "## **So now let's go back to the original CostÂ Function**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*a6HUYDO5WRfUu4dpcSkwaA.png)\n",
    "\n",
    "**Which in Vectorized Form for the Mean Squared Error is defined as below**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*hABj8H0iq2G-S1qC-pjirQ.png)\n",
    "\n",
    "And after calculating the Gradient of this MSE in Vectorized form, which we did above the Gradient-Descent Algorithm will update the **weights (Î¸ / Theta values** ) as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*ZHzoW49vH-a5c45lrjABSw.png)\n",
    "\n",
    "Compare the above with the Gradient-Descent formulae for the Numerical case\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*QhGYPsTKDMuN2XXV2-9tzA.png)\n",
    "\n",
    "\n",
    "## A manual example of the Gradient-Descent implementation\n",
    "\n",
    "Let's say for simple single variable training dataset we have the following values\n",
    "\n",
    "```\n",
    "x,y1,12,23,34,4\n",
    "```\n",
    "\n",
    "Further, assume,\n",
    "\n",
    "Î± (learning rate) = 1\n",
    "\n",
    "m (number of training examples) =4\n",
    "\n",
    "Setting _Î¸_0 to 0 and _Î¸_1 to 1\n",
    "\n",
    "So we have the linear equation\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*WbmMq0A-rS-DiWWWzWMVrQ.png)\n",
    "\n",
    "\n",
    "Now by convention,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*pcm_T-khvSgPtg3-vgjd1g.png)\n",
    "\n",
    "\n",
    "at the _i_âˆ’th row\n",
    "\n",
    "Further I denote,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*F0yXIAbfID8ZbIsnQicwdw.png)\n",
    "\n",
    "\n",
    "(i.e. after _k_ repetitions of the GD algorithm).\n",
    "\n",
    "So after a single update, with GD algorithm, i.e. applying the below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*ZHzoW49vH-a5c45lrjABSw.png)\n",
    "![](https://cdn-images-1.medium.com/max/800/1*qZniIowC3-qj4PM_MBXiCQ.png)\n",
    "\n",
    "\n",
    "So, regardless of how many times I apply the GD algorithm, the value of Î¸1 will be constantly equal to 1, since at every iteration we have _Î¸_0=0 and _Î¸_1=1\n",
    "\n",
    "#### Now extend the above to a multivariable case,\n",
    "\n",
    "Assume theta values have been picked at random as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*DEyr8pXY-YGRS0VCSDhkNQ.png)\n",
    "\n",
    "\n",
    "And the training-dataset is\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*JFmJrQcKJWhSO6qi5hShpw.png)\n",
    "\n",
    "\n",
    "So here, first, to calculate the hypothesis Equation, I need to transpose _Î¸_ to give our initial vector _Î¸_\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*ppUW7PoQ4khUZDd47M0GKw.png)\n",
    "\n",
    "\n",
    "And the GD-Algorithm is,\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*JPmxQrvKmjcAxiURYOCYUA.png)\n",
    "![](https://cdn-images-1.medium.com/max/800/1*2hJfGKtavtUlQKXF_mpVbw.png)\n",
    "\n",
    "\n",
    "And for applying the GD algorithm again, I need to evaluate\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*l06ChICjtfOkpXtnUZN5Tg.png)\n",
    "\n",
    "\n",
    "## Python/Numpy Implementation\n",
    "\n",
    "Please refer to the jupyter notebook\n",
    "\n",
    "First, **generate a training dataset** in Matrix form\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**NumPy zeros()** function in aboveâ€Š**â€”** you can create an array that only contains only zeros using the NumPy zeros() function with a specific shape. The shape is row by column format. Its syntax is as below\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*6Z-1jRbnX3VlRDGWEZzTKw.png)\n",
    "\n",
    "\n",
    "For example, the code to generate a Matrix of 2 by 3 (2 rows and 3 columns)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "A = np.zeros(shape = (2, 3))\n",
    "print\n",
    "\n",
    "\\# Output below\n",
    "\\[\\[0. 0. 0.\\]\n",
    " \\[0. 0. 0.\\]\\]\n",
    "\n",
    "Which produces an array like the following:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*qK3b2NWHDEB8p6hHbvB0gw.png)\n",
    "\n",
    "\n",
    "If I run the above **gen\\_data()** function above for a set of 5 training data-set as below with bias and variance of 20 and 10 respectively\n",
    "\n",
    "gen\\_data(5, 20, 10)\n",
    "\n",
    "I will have the following form of output\n",
    "\n",
    "(array(\\[\\[1., 0.\\],\n",
    " \\[1., 1.\\],\n",
    " \\[1., 2.\\],\n",
    " \\[1., 3.\\],\n",
    " \\[1., 4.\\]\\]),\n",
    " array(\\[22.38023816, 24.18406356, 28.01360908, 26.80051617, 29.30101971\\])\n",
    " )\n",
    "\n",
    "And now finally invoke the above 2 functions to create some linear data and run the gradient-descent and also plot it to a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# below function will update the Linear Regression\n",
    "# co-efficient beta and return the updated beta\n",
    "def gradient_descent(x, y, beta, alpha, m, iterations_num):\n",
    "    # Variable to hold  is the transpose of x\n",
    "    x_transposed = x.T\n",
    "\n",
    "    # .T and the transpose() call both return the transpose of the array.\n",
    "    # In fact, .T return the transpose of the array,\n",
    "    # while transpose() is a more general method, that can be given\n",
    "    # axes (transpose(*axes), with defaults that make the call transpose() equivalent to .T\n",
    "    # .T is just a convenient notation,\n",
    "\n",
    "    for i in range(0, iterations_num):\n",
    "        hypothesis = np.dot(x, beta)\n",
    "\n",
    "        # hypothesis - y is the first part of the square loss' gradient\n",
    "        # (as a vector form for each component), and this is set to the loss variable.\n",
    "\n",
    "        loss = hypothesis - y\n",
    "\n",
    "        cost = np.sum(loss ** 2) / (2*m)\n",
    "\n",
    "        # Now calculate the Vectorized Gradient from our earlier defined formulae\n",
    "\n",
    "        vectorized_gradient = np.dot(x_transposed, loss) / m\n",
    "\n",
    "        # now with above gradient update beta\n",
    "        beta = beta - alpha * vectorized_gradient\n",
    "\n",
    "        return beta\n",
    "\n",
    "\n",
    "# Now generate a Matrix form of training data\n",
    "# num_of_training_data_points => Number of rows of the generated Matrix\n",
    "# And the Matrix will have 2 features or coefficients\n",
    "def gen_data(num_of_training_data_points, bias, variance):\n",
    "    x = np.zeros(shape=(num_of_training_data_points, 2))\n",
    "    y = np.zeros(shape=num_of_training_data_points)\n",
    "\n",
    "    # above will generate a data for a straight line\n",
    "    for i in range(0, num_of_training_data_points):\n",
    "        # bias feature\n",
    "        x[i][0] = 1\n",
    "        x[i][1] = i\n",
    "\n",
    "        #our target variable\n",
    "        y[i] = (i + bias) + random.uniform(0, 1) * variance\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def plot_model(x, y, w):\n",
    "    plt.plot(x[:,1], y, \"x\")\n",
    "    plt.plot(x[:,1], x * w, \"r-\")\n",
    "    plt.show()\n",
    "\n",
    "def test_gradient_descent(model_function):\n",
    "    # gen 100 points with a bias of 20 and 10 variance representing noise in data\n",
    "    x, y = gen_data(100, 5, 2)\n",
    "    m, n = np.shape(x)\n",
    "    num_of_iterations = 100000\n",
    "    alpha = 0.0005\n",
    "    beta = np.ones(n)\n",
    "    w = model_function(x, y, beta, alpha, m, num_of_iterations)\n",
    "    plot_model(x, y, w)\n",
    "\n",
    "test_gradient_descent(gradient_descent)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Reference and FurtherÂ Reading\n",
    "\n",
    "**Matrix Multiplication**â€Šâ€”â€Š[https://en.wikipedia.org/wiki/Matrix\\_multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)\n",
    "\n",
    "**Matrix-Calculusâ€Šâ€”** [https://en.wikipedia.org/wiki/Matrix\\_calculus](https://en.wikipedia.org/wiki/Matrix_calculus)\n",
    "\n",
    "**Vector\\_Field**â€Šâ€”â€Š[https://en.wikipedia.org/wiki/Vector\\_field](https://en.wikipedia.org/wiki/Vector_field)\n",
    "\n",
    "**Matrix Transpose Properties** -[https://en.wikipedia.org/wiki/Transpose#Properties](https://en.wikipedia.org/wiki/Transpose#Properties)\n",
    "\n",
    "**Matrix Cookbookâ€Šâ€”** [https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)\n",
    "\n",
    "**Online Calculation of Matrix Derivative**â€Šâ€”â€Š[http://www.matrixcalculus.org/](http://www.matrixcalculus.org/)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}